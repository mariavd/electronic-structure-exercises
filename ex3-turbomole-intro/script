#!/bin/bash
#SBATCH --job-name=test                  // Job name to be displayed in queue
##SBATCH --workdir=<Workir path>                // This defines the Slurm working directory
#SBATCH --output=foobar.out              // Job output at the completion
#SBATCH -p test                          // Request test partition
#SBATCH -c 1                             // Request single core
#SBATCH -e foobar.err                    // Define error file
##SBATCH --mail-type=END                  // Defining END of job mail notification
##SBATCH --mail-user=user@address.mail    // mail recipient
SDIR=`pwd`
#
# activate one of the two lines below in order to sellect
# the type of parallelization. aoforce, escf and egrad works only
# with SMP (within a single node).
#
#export PARA_ARCH=MPI
export PARA_ARCH=SMP
export PARNODES=$SLURM_NPROCS
export TURBODIR=/proj/group/compchem/TURBOMOLE-7.3.1
export PATH=$TURBODIR/bin/`$TURBODIR/scripts/sysname`:$PATH

export TMPDIR=/wrk/group/compchem/tmp
export TURBOTMPDIR=/$TMPDIR/$SLURM_JOB_ID/turbotmpdir

# remove possible earlier $tmpdir from control file
kdg tmpdir

#
# start job

#actual -r
/usr/bin/time -v  dscf > dscf.out
#/usr/bin/time -v  ricc2 > ricc2.out
#/usr/bin/time -v jobex -ri -c 500 > jobex.out
# print som info like used time, disk, reserved memory and actual used memory
sacct -a --format=elapsed,maxdiskwrite,reqmem,maxrss  -j $SLURM_JOB_ID

